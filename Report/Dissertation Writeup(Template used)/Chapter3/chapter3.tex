\begin{flushleft}

\chapter{Implementation}
\label{Implementation}

\section{Early Development}
After the first weeks where background reading were done and designing a plan for the final program, work using the Webots\textsuperscript{\texttrademark} simulator was started. \\ More information about the different stages can be found in the following sub-sections. 

\subsection{Webots Tutorials}
In the beginning different tutorials and demo following  programs included in the Webots installation were followed. 
This helped to see how the simulator works, what it can do and what possibilities the Webots API gives an developer. \\
The tutorials used were provided by the Webots wikibooks site \footnote{\url{http://en.wikibooks.org/wiki/Cyberbotics\%27_Robot_Curriculum}}.\\
Starting off with simple tutorials and movement and sensor reading the developer was soon able to implement simple programs which were based on simple forwards movement with obstacle avoidance functions, based on the proximity sensor values. 
After some of the more "advanced"(in comparison to what had been done to this point) tutorials which involved e.g.: reading the robot encoders look at example programs of more advanced types of movement as well as SLAM examples were taken. \\

\subsection{First Program Iterations}
After that the experience of the tutorials was taken and it was started on to actually implementation the first prototype of the program. \\
The first prototype was still heavily based on a Webots example program called "Intermediate Lawn Mower" which involved a simple movement pattern based on a basic finite state machine(FSM). \\ 
The first attempt was based recreating the FSM to get the same basic movement capabilities as the the program it was based upon and than changing it to achieve the level of movement control which was needed. \\[3ex]

In the following iteration it was tried to implement the functionality of moving a given distance based on resetting the stepper motors encoders and moving the robot until it reaches a given encoder value. It was quickly realised that this would not work with the way the FSM was currently implemented so it was decided to move the functionality of the FSM, which was currently based on a simple switch statement, to a set of separate functions. The idea was that this would give the freedom needed to be able to call a method, i.e. \textit{move\_forward()},  and keep it running until a predefined encoder value has been reached. \\
However this did not work based on the overall way the program was designed at this point. It was designed around an switch statement which based its decisions on the proximity sensors of the E-Puck and then setting the movement speed for the robot motors. As it was based on this calling separate functions to move and turn did not work, which resolved in problems around the current idea of using the stepper motor encoders. \\[3ex] 

As it was needed to be able to move a certain distance in order to effectively implement the rectangular movement pattern, various approaches which were all based around the FSM were tried. 
It was realised soon that this was not getting anywhere which this way of thinking so it decided to come back to this problem later and went to another problem: turning a given number of degrees. \\
As the simulator simulates friction between the robot wheels and the environment the turning function, as it was in it current state, was far to inaccurate to be usable. \\
It was based on a very basic odometry calculation, taking the encoder values and calculating the turning distance based on the wheel diameter and axle length of the robot. While this is not a bad approach it had no procedures of slowing down the movement as it got closer to the target state so overshooting the wanted position or rotating far to less if the motor speed would have been set to low. \\
It was tried to counter the problem by alternating the values it used for the odometry calculations, and while it came close to a solution it was far from accurate. Another problem with this solution was that it only worked for ~90 degree rotations based on the modified values, meaning all rotations to another point were impossible without modifying the values to fit the target heading. Which was counter productive as it would be best to have 1 function able to orientate the robot to any wanted heading. \\

\section{Mid stage Development}
After the problems which were encountered during the first program iterations, it quickly realised that this way of thinking and understanding of the API was flawed. A lot of the problems which were encountered were the result of insufficient programming skills for what  was tried to do combined with bad understanding of how odometry worked, and how this could be used it to control the robot.\\
Seeing these kinds of problems it was decided to take a step back as the thought process for the program design was clearly "moving in circles" and encountering the same kind of problems over and over again with the current approach. \\
It was decided to take once more a look at the provided example programs and to study their odometry functions in order to gain some new insight into odometry calculations, and find a new approach based on that.

\subsection{Advanced tutorials}
The example programs provided together with the simulator are categorised after the estimate level of knowledge needed to complete them. Theses categories are: Beginner, Novice, Intermediate and Advanced. The advanced programs were already looked at before as they feature a couple of examples on odometry and slam, however they were not understood  well enough at the time.\\ 
Looking closer at the odometry functions provided it was managed to understand a bit better how the advanced programs worked and how they calculated the movement of the robot. \\[3ex]

A look was also taken at the code and notes which were taken during the Robotics module on the second year, while the API worked different for the Player/Stage environment the idea behind rotation and movement was still the same and had only to be applied using the Webots API.

\section{Movement}
After having studied the odometry functions of the provided programs it was decided to start a new approach to fix the movement of the robot.\\
This approach is based on a set of different functions, much smaller and more refined than my previous approach. This approach took a while to implement and test but the results were rather satisfactory. This section is going to describe the different aspects of the movement solution and describe how the most important functions work.\\
The code of the major functions will be added as appendices, the site numbers will be added to each subsection.

\subsection{Moving Forward}
\label{moving_forward_description}
The code for this function can be found in Appendix B, section \ref{moving_forward_code} at page \pageref{moving_forward_code}.\\
The \textit{move\_forward }function takes 2 doubles as parameters, 1 being the speed with which the robot is ordered to move and the distance it should move. The last parameter is a link to the global odometry struct, this struct is used to update the odometry values.\\
The function first checks that neither of the parameters are 0 and then calculates the number of steps each motor has to drive(1 step being 1 step of the stepper motor) to reach its target position.
This calculation is done by dividing the number of steps needed for a full wheel rotation, 1000 in this case, by the product of $\pi$ times the wheel diameter times 2 and multiplication this result with the distance defined in the parameter of the function. The steps needed for a full rotation have been taken from the E-Puck documentation and have been confirmed during on of the odometry tutorials. \\
It will then read the current encoder positions and calculate the stop position for each motor by the sum of the encoder values for each motor and the previously calculated encoder steps needed to reach the target area. 
It will then set the motor speed based on the speed defined in the parameters.\\[3ex]

It then enters the control code which will stop the robot once it reaches it target, position. \\
This is controlled by comparing the current encoder positions with the calculated target encoder positions and updating them all the time. 
Once the robot reached a pre-defined minimum distance of 20 encoder steps it will slow down the movement to a minimum speed of 10 steps per second. This is down to prevent the robot from overshooting the target area should it move with to much speed. The optimal minimum distance and speed has been found experimentally, and both values give good results and also prevent the robot from undershooting.
Once it has reached it target location it will stop the robot, and force the simulator to take a simulation step, effectively moving onward to the next command.\\[3ex]

This function allows the robot to move forward and stop after the predefined distance within a minimal error space, which will always exist given the friction simulated inside the simulator. This method required a lot of testing in order to get right as first versions did not include the control statement which slowed down the robot after a minimum difference between the encoders and the target encoder value has been reached. So the robot used to overshoot the target. \\
After the control statement was implemented it still required some testing and calibration of the minimum difference and speed values in order to avoid over and undershooting. However the found values work well and the movement error has been reduced to minimum.\\

\subsection{Turn a given angle}
\label{turn_angle_description}
The code for this function can be found in Appendix B, section \ref{turning_angle_code} at page \pageref{turning_angle_code}.\\
The turn\_angle function takes 2 doubles as parameters, one being the angle the robot will turn to the other the speed with which the robot will turn.\\
First the factor by which the robot will turn is calculated by dividing 360, the value of a full rotation, with the defined angle.
Once the factor has been calculated it will then the number of steps the motor have to do until the target position is reached. This is done by dividing the product of the steps needed for a full wheel rotation, 1000, and the size of the wheelbase by the product of the calculated turning factor and 2 times the wheel radius. It will then use a function to return the current motor encoder positions. This function simply uses the Webots\textsuperscript{\texttrademark}  API and returns the values. \\[3ex]

If the rotation angle defined as the function parameter is positive the robot will turn to the right.\\
When the robot is turning to the right it will calculated the stopping positions of the encoders by adding the calculated step count to the left motor encoder and subtracting it from the right encoder. This will lead to the wheels turning against each other and will result in the robot turning on the spot rather than only moving 1 wheel to turn which would result in a displacement of the robot. \\
It will then update the global odometry buffer by calling the \textit{compute\_odometry\_data} function and set the the speed of the motors using the given function parameter value. The right motor will receive a negated value so that it will turn backwards. 
It will then compare the left encoder positions and updated them all the time. Similar to how the forward movement function worked, it will detect when a given minimum difference between the current and target encoder values is reached and slow the robot down to a minimum speed. \\[3ex]

If the rotation angle defined as the function parameter is negative the robot will turn to the left.\\
The only difference between turning left rather than to the right is that the calculations, obviously, are reversed. Meaning to calculated the stop positions of the motor encoders it will subtract the calculated step count from the current left encoder value and add the step count the the right
encoder value, same switch of negation has been done where the motor speeds are set.
The calculations of how long to turn and when to slow down are identical to how they work when turning right, only difference being that the the operators to which check how long to turn are different.\\
Once the target position has been reached, by either turning left or right, the robot stops and the global odometry buffer is updated. 
It will then force to the simulator to take a simulator step, effectively moving on to the next command.\\[3ex]

This functions allows me to define the turn the robot by so many degrees as I need and it will turn there within an minimal error space. This error space exist because the simulator simulates friction between the robot wheels and the environment so 100\% accurate movement will never happen.\\
There also existed the problem of over/undershooting with the turning however the values found during tests of the \textit{move\_forward} function turned out to also work well for the turning function. However one problem remains, since there never is going to be a perfect rotation the error value will add up over time, resulting in less and less accurate turns, overshooting the target rotation is going to be a real problem. I have at this point not yet a solution for this problem, however the function works well and is a great improvement to how it turning was implemented in previous iterations of the program.

\section{Localisation using Odometry}
After the studying the optometry functions provided and implementing the movement algorithms, it was time to implement the localisation using odometry calculations.\\
The odometry functions which were implemented are used for localisation the robot inside the environment and finding it heading. The functions only require the starting point and localisation of the robot, and are then able to calculate the movement and rotation of the robot with every movement done, within a certain degree of accuracy. The uncertainty in accuracy is based on the friction which get simulated inside the simulator. \\
These functions are largely similar to the ones provided with the Webots\textsuperscript{\texttrademark} interface, however some minor changes has been done. \\
Similar to the way in which the movement algorithms have been described the odometry functions are going to be described. 

\subsection{Initialising the Odometry algorithms}
\label{odometry_init_description}
The code for this function can be found in Appendix B, section \ref{odometry_init_code} at page \pageref{odometry_init_code}.\\
The code for the odometry struct can be found in Appendix B, section \ref{odometry_struct_code} at page \pageref{odometry_struct_code}.\\
To initialize the odometry algorithms, 2 functions are used. \\
The first function, \textit{odometry\_track\_start} takes a \textit{odometrtyTrackStruct}, which is defined in the class which calls the function, as parameter. It will then acquire the encoder positions of the robot and call the next function, \textit{odometry\_track\_start\_pos} which set's the starting values, including the encoder positions inside the odometry struct.\\[3ex]


Also the distance travel when a wheel turns and the wheel conversion are calculated, used for this are parameters acquired during calibrations done in the Webots \textsuperscript{\texttrademark} example programs(the values are the same as it is the same virtual robot model) and the E-Puck documentation. 

\subsection{Updating the Odometry values}
\label{odometry_update_description}
The code for this function can be found in Appendix B, section \ref{odometry_update_code} at page \pageref{odometry_update_code}.\\
To update the odometry values of the struct the function \textit{odometry\_track\_step} is called.
When this function is called inside another function a number of things happen.\\
The current encoder positions for the stepper motors are fetched, 
and used as a parameter inside the \textit{odometry\_track\_step\_pos} function call. \\
Inside this function the new X, Y coordinates and the rotation of the robot are calculated. This is achieved by first calculating the difference between the current encoder positions and the encoder previous encoder positions saved inside the struct. 
This difference is then multiplied by the wheel conversion, which gets calculated inside the initialization step. \\
The result of this is used to calculated the wheel movement of the left and right wheel, results which are used to calculate the rotation of the robot. 
The next step is to calculate the new X and Y coordinates of the robot, based on the sum of done left and right wheel movement  and math calculations using the robots rotation. 
At the end the calculated X and Y coordinates as well as the rotation value are used to update to struct. And the current encoder positions are saved inside the buffer for later calculations. 

\subsection{Minimum speed and distance calibration}
After these functions were created they were calibrated using following processes .\\

\begin{figure}[h]
\centering
\includegraphics[width = 0.5\textwidth]{../../figures/movement_test.png} 
\caption{Movement algorithm test pattern}
\label{movement_test}
\end{figure}

Figure \ref{movement_test} shows a simple movement pattern which was used to calibrate the minimum turn distances and tested that the \textit{move\_forward()} and \textit{turn\_angle()} methods worked as intended. \\
How it works is the E-Puck starts on 1 corner of a square(here the floor of the simulator was used as it has chessboard representation) and move forwards to the other end of the square, turn 90 degrees and so on until it reaches it start point. It then turns around and follows the same pattern back to its original starting position. This tests allows the notice of odometry error in the rotations and forward movement, after a while it would turn/move to far or not far enough and thereby not reach it accurate start point. Noticing these errors allowed for calibration of the speed and distance where the algorithms will slow done the movement to minimize this error as much as possible, see section \ref{moving_forward_description} and \ref{turn_angle_description} respectively for more information on this.\\[3ex]
The test is based on the "University of Michigan Benchmark" or UMBmark\footnote{\url{http://www.cs.columbia.edu/~allen/F13/NOTES/borenstein.pdf}}.\\[3ex]

The problem with this calibration at the time was rather than using this approach to calibrate the effective wheelbase and the wheel radius at the time of this calibration the angles to turn were changed.\\
This led to rather than updating the wheelbase from  the original 0.052(which has been done in a later stage of the project. \textbf{Note: the code which can be found in Appendix B uses the updated wheelbase}.\\
Using the non-updated wheelbase it was required to change specify an angle of 100$^{\circ}$ to move 90$^{\circ}$ effectively.

\section{First results}
After the calibration process of the previous section a new program iteration was started, using the new movement and localisation algorithm in order to test the mapping and continue implementing it. \\[3ex]

This first implementation was not using the deployment pattern but rather simply following the walls, this was done so that the mapping could be tested. Rather than having an advanced wall following algorithm at this stage a simple collision avoidance algorithm was used which turned the robot by 90$^\circ$ when ever an obstacle is reached. 

\subsection{Early mapping methods and results}

\begin{lstlisting}[caption={Proximity sensor reading}]
//distance sensors array definitions
int ps_value[NUM_DIST_SENS]={0,0,0,0,0,0,0,0};
int obstacle[NUM_DIST_SENS]={0,0,0,0,0,0,0,0};
int ps_offset[NUM_DIST_SENS] = {35,35,35,35,35,35,35,35};

// obstacle will contain a boolean information about a collision
for(i=0;i<NUM_DIST_SENS;i++){
	ps_value[i] = (int)wb_distance_sensor_get_value(ps[i]);
	obstacle[i] = ps_value[i] - ps_offset[i] > THRESHOLD_DIST;
}

//define boolean for sensor states for cleaner implementation
bool ob_front =
	obstacle[PS_RIGHT_10] ||
	obstacle[PS_LEFT_10];

bool ob_right =
	obstacle[PS_RIGHT_90];

bool ob_left =
	obstacle[PS_LEFT_90];
\end{lstlisting}

The code above shows the code used at this stage of the project to detect obstacles, and set boolean values if an obstacle is detected in the direction of the  obstacle. As the test environment for this part of the project is a simple square area the turning of 90$^{\circ}$ is sufficient and no other control was needed at this stage to test the mapping. 

\begin{figure}[h]
\centering
\includegraphics[width = 0.5\textwidth]{../../figures/e_puck_sensor_placement.png} 
\caption[E-Puck sensor placement]{E-Puck sensor placement\footnotemark}
\label{sensor_placement}
\end{figure}

Figure\footnotetext{Figure \ref{sensor_placement} is taken from: \url{http://www.cyberbotics.com/cdrom/common/doc/webots/guide/section8.1.html}}  \ref{sensor_placement} show the sensor placement on the E-Puck robot. Its is worth noticing that for sake of simplicity and better work flow the sensors have been renamed in the code. The new naming scheme does show which side of the robot the sensor is placed and its placement in degrees, relevant to the forward facing center of the robot. As an example the sensor ps0 in figure \ref{sensor_placement} is named PS\_RIGHT\_10 in the code since it is placed on the right side of the robot with an orientation of 10$^{\circ}$ relevant to the center.
Compared to this sensor ps5 is PS\_LEFT\_90.   \\[3ex]

The mapping algorithm used a similar approach to the obstacle detection algorithm. \\
One of the problems which were noticed at this stage was the problem of the noise simulated inside the simulator, this leads to the mapping algorithm needing a high threshold in order to map only actual obstacles, and not random noise spikes.  This however leads to the problem of the robot needing to almost be in direct contact with the obstacles in order to map them. For the mapping at this stage this was not a problem however this strongly influenced movement pattern decisions at a later stage in the implementation process, more about that in a later section.

\begin{figure}[h]
\centering
\includegraphics[width = 0.5\textwidth]{../../figures/early_map.jpg} 
\caption{Early mapping result}
\label{early_mapping_result}
\end{figure}

Figure \ref{early_mapping_result} shows clearly the strongest problems with this solution so far as well as demonstrates the need to have a good solution for  the main aspects of this project, localisation and deployment.\\
As it can clearly be seen in the figure, the localisation and movement approach were far from perfect at this stage in the project. 
While the program generates a closed  map the mapping is strongly skewed to the left side. This error was strongly based on odometry errors during turning caused by sup-optimal calibrated robot values as well as friction between the wheels and the floor.\\
At the time to movement algorithm did not update the odometry struct, and the struct were only updated once every loop iteration. \\
This was the stage of the project at the time of the mid-project demo.


\subsection{Direction control based on heading}
The approach done to fix the odometry error at this stage was done by specifying directions in which the robot is moving as north, east, south and west. 

\begin{figure}[h]
\centering
\includegraphics[width = 0.5\textwidth]{../../figures/direction_and_angles} 
\caption{Directions and their corresponding Angles}
\label{directions_and_angles}
\end{figure}

Figure \ref{directions_and_angles} does show the directions used and the corresponding angles. The directions were represented in the code by using boolean values. \\
In order to get the direction controlled movement working a number of functions needed to be created.  

\begin{lstlisting}[caption={Converting Radians to degrees}]

#ifndef M_PI
	#define M_PI 3.1415926535897932384626433832795L
#endif

#define RTOD(r) ((r) * 180 / M_PI)

/**
returns the angle in which the robot is moving
*/
int return_angle(double rad){
	double rotation;

	if(RTOD(rad) < 0){
		rotation = RTOD(rad) + 360;
	}else{
		rotation = RTOD(rad);
	}
	printf("%f\n", rotation);
	return rotation;
}
\end{lstlisting}

This function converts radians, which are used by the simulator to degrees, this was done to make it easier to work with.
The next function uses this to check in which direction the robot is moving and set a boolean value representing a direction to true.

\begin{lstlisting}[caption={Early check of the movement direction}]
#define ANGLE_TOLERANCE 20
#define EAST 0
#define NORTH 90
#define WEST 180
#define SOUTH 270

//direction definitions
bool north,west,south,east;

/**
set booleans for the direction the robot is moving in
*/
void check_direction(double d){
	int i = return_angle(d);
	east = false;
	north = false;
	west = false;
	south = false;

	if(EAST < i + ANGLE_TOLERANCE || EAST > i - ANGLE_TOLERANCE){
		east = true;
	}else if(NORTH < i + ANGLE_TOLERANCE || NORTH > i - ANGLE_TOLERANCE){
		north = true;
	}else if(WEST < i + ANGLE_TOLERANCE || WEST > i - ANGLE_TOLERANCE){
		west = true;
	}else if(SOUTH < i + ANGLE_TOLERANCE || SOUTH > i - ANGLE_TOLERANCE){
		south = true;
	}
}
\end{lstlisting}

At this moment the deployment pattern described in chapter \ref{Design}  was also implemented. \\
This was done by checking which direction the robot is facing when an obstacle is found in front of the robot. At this point a couple of procedures were designed for what to do for each direction. Effectively the robot is performing u-turns when ever an obstacle is found directly in front of it. 

\begin{figure}[h]
\centering
\includegraphics[width = 0.5\textwidth]{../../figures/direction_uturn_pattern} 
\caption{Directions and their corresponding U-Turn pattern}
\label{directions_uturn_pattern}
\end{figure}

The u-turn patterns shown in figure \ref{directions_uturn_pattern} effectively led the robot to move from right to left and from the top downward, there wasn't any particular reason for this, other than an uniformed pattern was needed. \\ 
The next piece of code is a snipped from the deployment loop at that time in the project. It uses the previous shown \textit{check\_direction()} as well as the \textit{controll\_angle()} method which will be shown at a later point.\\
The control angle function is used to compare the current heading with the heading specified by the general direction the robot is moving in(north/east/south/west). 
As the code snipped shows at this point the the odometry struct is still not updated during the deployment loop, and the heading is only checked and corrected in the FORWARD state of the loop. 
The \textit{turn\_right()} and \textit{turn\_left()} functions turn the robot 90$^{\circ}$.

\begin{lstlisting}[caption={Early Control loop snipped}]
double dMovSpeed = 500.0f;
double dDistance = 0.01f;
double dTurnSpeed = 100.0f;
double dTurnDistance = 0.05f;	

switch(state){
	case FORWARD:
		move_forward(dMovSpeed, dDistance);
		controll_angle(&ot);
		if(ob_front){
			state = STOP;
		}
		break;	
	case STOP:
		stop_robot();

		if(ob_front && ob_left){
			state = TURNRIGHT;
		}
		else if(ob_front && ob_right){
			state = TURNLEFT;
		}
	else if(ob_front){
		check_direction(ot->result.theta);
		state = TURNRIGHT;
	}
	break;	

	case TURNRIGHT:
		turn_right(dTurnSpeed);
		state = FORWARD;
		break;
	case TURNLEFT:
		turn_left(dTurnSpeed);
		state = FORWARD;
		break;
	case UTURN:
		if(north){
			turn_left(dTurnSpeed);
			move_forward(dTurnSpeed, dTurnDistance);
			turn_left(dTurnSpeed);
			state = FORWARD;
		}else if(south){
			turn_right(dTurnSpeed);
			move_forward(dTurnSpeed, dTurnDistance);
			turn_right(dTurnSpeed);
			state = FORWARD;
		}else if(west){
			turn_left(dTurnSpeed);
			move_forward(dTurnSpeed, dTurnDistance);
			turn_left(dTurnSpeed);
			state = FORWARD;
		}else if(east){
			turn_right(dTurnSpeed);
			move_forward(dTurnSpeed, dTurnDistance);
			turn_right(dTurnSpeed);
			state = FORWARD;
		}
		east = false;
		north = false;
		south = false;
		west = false;
		break;
	default:
		state = FORWARD;
}
\end{lstlisting}

It is worth mentioning that this approach went through a number of modifications before this state was reached. The previous modifications were done to check to optimal placement of the \textit{controll\_angle()} function. It was found to be unproductive to have the check after every turn, as it did not matter on the small distances the robot was moving in the u-turn process. \\

\begin{lstlisting}[caption={Early heading correction algorithm}]
 /**
Controll the movement angle
*/
void controll_angle(struct odometryTrackStruct *ot){
	int rotation;
	int dSpeed = 100.0f;
	int corAngle = 5.0f;

	if(RTOD(ot->result.theta) < 0){
		rotation = RTOD(ot->result.theta) + 360;
	}else{
		rotation = RTOD(ot->result.theta);
	}

	//check movement to the right(east)
	if(EAST < rotation + ANGLE_TOLERANCE){
		turn_angle(-corAngle, dSpeed);
	}else if(EAST > rotation - ANGLE_TOLERANCE){
		turn_angle(corAngle, dSpeed);
	}

	//check movement to the north
	if(NORTH < rotation + ANGLE_TOLERANCE){
		turn_angle(-corAngle, dSpeed);
	}else if(NORTH > rotation - ANGLE_TOLERANCE){
		turn_angle(corAngle, dSpeed);
	}

	//movement to the west
	if(WEST < rotation + ANGLE_TOLERANCE){
		turn_angle(-corAngle, dSpeed);
	}else if(WEST > rotation - ANGLE_TOLERANCE){
		turn_angle(corAngle, dSpeed);
	}

	//movement to the south
	if(SOUTH < rotation + ANGLE_TOLERANCE){
		turn_angle(-corAngle, dSpeed);
	}else if(SOUTH > rotation - ANGLE_TOLERANCE){
		turn_angle(corAngle, dSpeed);
	}
}
\end{lstlisting}
The code above is an early version of the algorithm which checks and corrects the heading the robot is moving in. This algorithm was used to correct the odometry error caused by the friction in the environment and the, at this stage, sub-optimal calibrated robot attributes. \\
It worked by comparing the heading the robot is heading in with the wanted heading and correcting any errors. The algorithm at this stage has some short comings such as it is not possible to turn the robot more accurate than with a 5$^{\circ}$ threshold. If the threshold was reduced to less than  5$^{\circ}$, the robot would constantly overturn. \\[3ex]

At this time it was assumed that this inaccuracy was caused by the friction simulated inside the simulator, and that this movement was as accurate as it was going to be. \\
The odometry error at this stage did improve thanks to the \textit{control\_angle()} functions, the start of the mapping was much less skewed than the previous approach, which result can be seen in function \ref{early_mapping_result} at page \pageref{early_mapping_result}. \\[3ex]

However the constant odometry error did accumulated over time, so that the location displacement became to big to able to map the environment effectively. \\
In order to try to fix the localisation error, a couple of approaches were done in order to reduce said error. Before those approaches were done some refactoring and code improvement was done.

\section{Refactoring and improvement}
In this section the refactoring and code improvements which were done at this stage of the project is described. \\
Some of the refactorings I have not addressed in this section are the movement of functions to other source files.
While it clearly is good practise to create functions at the right place in the first place, a lot of prototype functions were created in the main source files for sake of simplicity during the first functions test. These functions were later moved to other source files were they are more appropriate, and to keep the overall source file size done. 

\subsection{Deployment algorithm refactoring}
Before other approaches were tried it was decided that the deployment algorithm needed to refactored to make the code cleaner and easier. \\
The following piece of code is a snipped taken from one of the algorithm:

\begin{lstlisting}[caption={Deployment algorithm refactoring}]
double dSpeed = 500.0f;
double dDistance = 0.01f;

char no[] = "north";
char ea[] = "east";
char we[] = "west";
char so[] = "south";

case UTURN:
	if(north){
		printf("%s\n", no);
		turn_left(dTurnSpeed);
		for(it = 0;it < 5;it++){
			move_forward(dSpeed, dDistance);
		}
		turn_left(dTurnSpeed);
		north = false;
		state = FORWARD;
	}else if(south)[...]
\end{lstlisting}

As the above snipped shows the unneeded variables of \textit{dTurnSpeed} and \textit{dTurnDistance} were removed as it was decided that the algorithm, for simplicities sake, use a single value type which can easily be changed. In order to get the same amount of movement during the u-turn phase a simple \textit{for loop} is used. The same modifications as are shown in the code snipped have been done to the rest of the u-turn \textit{case}. \\
In order to improve the code further, printout statements were added to the u-turn routine as well as to the \textit{return\_angle()} function.
This needed to be done as the webots simulator does not have a debugger, so printouts were used to track to results of the algorithms.

\subsection{Deployment algorithm improvement}
\label{deployment_improvement}
The previous u-turn method did not support mapping obstacles while the u-turn was performed. As this is not optimal behavior the u-turn pattern was altered again, to support the mapping methods. 

\begin{lstlisting}[caption={U-turn improved with obstacle detection and mapping}]
#define NUM_DIST_SENS 8 //number of distance sensors 
#define OCCUPANCE_DIST 150 //obstacle mapping threshold
double dSpeed = 500.0f;
double dDistance = 0.01f;

static int wtom(float x){
	return (int)(MAP_SIZE / 2 + x / CELL_SIZE);
}

robot_x = wtom(ot->result.x);
robot_y = wtom(ot->result.y);

char no[] = "north";
char ea[] = "east";
char we[] = "west";
char so[] = "south";

case UTURN:
	if(north){
		printf("%s\n", no);
		turn_left(dSpeed);
		for(it = 0;it < 5;it++){
			move_forward(dSpeed, dDistance);
			
			//mark cells as occupied
			wb_display_image_paste(display,background,0,0);
			wb_display_set_color(display,0x000000);
			for(i = 0;i < NUM_DIST_SENS;i++){
				if(wb_distance_sensor_get_value(ps[i]) > OCCUPANCE_DIST){
					occupied_cell(robot_x, robot_y, ot->result.theta + angle_offset[i]);
				}
			}
			wb_display_image_delete(display,background);
			background = wb_display_image_copy(display,
				0,0,display_width,display_height); 
		}
		turn_left(dSpeed);
		north = false;
		state = FORWARD;
	}else if(south){[...]
\end{lstlisting}

The above code shows how the obstacle detection and mapping were added to the u-turn \textit{case}. 
This map's the obstacles to the sides of the E-Puck while the robot moves along the side of it and proceeds to the next turn. While this mapping is not good, as it is very "spotty".

\begin{figure}[h]
\centering
\includegraphics[width = 0.5\textwidth]{../../figures/map_results/dotted_uturn_mapping.png} 
\caption{Dotted mapping during U-Turns}
\label{dotted_uturn}
\end{figure}

Figure \ref{dotted_uturn} shows the dotted mapping which is caused during u-turns. While it is in no form a good representation of the environment it provides an outline of obstacles. The problem with this is however that because of the u-turn pattern it is needed for the robot to traverse a long each obstacle / wall in order to map them correctly. 

\section{Mapping}
In this section the mapping algorithm are shown and described. \\
The mapping algorithms are one of the key algorithms of the project, however based on code from the Webots\textsuperscript{\texttrademark} demo programs It is based on a occupancy map, which is a simple 2D map. The function used to mark a \textit{cell} of the occupancy map.\\
The code for this function can be found in Appendix A, section \ref{occupied_cell} on page \pageref{occupied_cell}.
The parameters for the function represent the X and Y coordinates as well as the rotation of the robot. These values are received from the odometry struct. 
The functions uses the Webots\textsuperscript{\texttrademark}  API to draw the cell in form of a 1x1 pixel rectangle on the display. 

\begin{lstlisting}[caption={obstacle detection and mapping}]
#define NUM_DIST_SENS 8 //number of distance sensors 
#define OCCUPANCE_DIST 150 //obstacle mapping threshold

static int wtom(float x){
	return (int)(MAP_SIZE / 2 + x / CELL_SIZE);
}


robot_x = wtom(ot->result.x);
robot_y = wtom(ot->result.y);

wb_display_image_paste(display,background,0,0);
wb_display_set_color(display,0x000000);
for(i = 0;i < NUM_DIST_SENS;i++){
	if(wb_distance_sensor_get_value(ps[i]) > OCCUPANCE_DIST){
		occupied_cell(robot_x, robot_y, ot->result.theta + angle_offset[i]);
	}
}
wb_display_image_delete(display,background);
background = wb_display_image_copy(display,0,0,display_width,display_height); 
\end{lstlisting}

The code above shows for loop which checks the value reported by each 1 of the 8 distance sensors and compare this value to the defined threshold. If the value is larger than the threshold the cell will be marked. 









 






\end{flushleft}
